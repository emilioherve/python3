{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01988880",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35213,
     "status": "ok",
     "timestamp": 1648247172838,
     "user": {
      "displayName": "NDOUMBE Emile Herve",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09643065439420591964"
     },
     "user_tz": 240
    },
    "id": "01988880",
    "outputId": "5cb3a79b-fe2f-4b21-907d-41ae8538cb16"
   },
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install functools\n",
    "!pip install varname\n",
    "!pip install statsmodels\n",
    "!pip install sklearn\n",
    "!pip install seaborn\n",
    "!pip install xgboost \n",
    "!pip install scikit-optimize\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10f0186",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2465,
     "status": "ok",
     "timestamp": 1648247213354,
     "user": {
      "displayName": "NDOUMBE Emile Herve",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09643065439420591964"
     },
     "user_tz": 240
    },
    "id": "e10f0186",
    "outputId": "13e7433d-4b5b-4a73-d978-071fd616d107"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask in /usr/local/lib/python3.7/dist-packages (2.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install dask\n",
    "import dask as dd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f83fba",
   "metadata": {
    "id": "37f83fba"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a96e58ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3712,
     "status": "ok",
     "timestamp": 1648247235025,
     "user": {
      "displayName": "NDOUMBE Emile Herve",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09643065439420591964"
     },
     "user_tz": 240
    },
    "id": "a96e58ab",
    "outputId": "90ca3459-94a6-4869-cb34-cabff4d28e00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from varname import nameof\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels import robust\n",
    "from statsmodels.tsa.api import VAR\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "\n",
    "# import a Lasso module for time series analysis\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LassoCV, RidgeCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# import Random Forest module\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# import XGBoost module\n",
    "from xgboost import XGBRegressor\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from functools import partial\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Bidirectional, LSTM, Dropout, SimpleRNN\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_decomposition import PLSRegression, PLSSVD\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "#import scipy.optimize as spop\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "import multiprocessing as multi\n",
    "warnings.filterwarnings(action='ignore',category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5271caf3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16794,
     "status": "ok",
     "timestamp": 1648247262923,
     "user": {
      "displayName": "NDOUMBE Emile Herve",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09643065439420591964"
     },
     "user_tz": 240
    },
    "id": "5271caf3",
    "outputId": "9f380b16-297d-4f16-d37c-5f056231de00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e3e42",
   "metadata": {
    "id": "936e3e42"
   },
   "outputs": [],
   "source": [
    "path1 =  '/content/gdrive/MyDrive/garch_month'                   # use your path\n",
    "all_files = glob.glob(path1 + \"/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54173a0c",
   "metadata": {
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1648247274425,
     "user": {
      "displayName": "NDOUMBE Emile Herve",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09643065439420591964"
     },
     "user_tz": 240
    },
    "id": "54173a0c"
   },
   "outputs": [],
   "source": [
    "path2='/content/gdrive/MyDrive/filemonth'\n",
    "files=glob.glob(path2 +\"/*.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Qoqd1ct8YEuk",
   "metadata": {
    "id": "Qoqd1ct8YEuk"
   },
   "outputs": [],
   "source": [
    "##### Implement Garch(1,1) function\n",
    "\n",
    "def garch_mle(params):\n",
    "    mu_0=params[0]\n",
    "    mu_1=params[1]\n",
    "    mu_2=params[2]\n",
    "    mu_3=params[3]\n",
    "    mu_4=params[4]\n",
    "    mu_5=params[5]\n",
    "    mu_6=params[6]\n",
    "    mu_7=params[7]\n",
    "    mu_8=params[8]\n",
    "    mu_9=params[9]\n",
    "    mu_10=params[10]\n",
    "    omega=params[11]\n",
    "    alpha=params[12]\n",
    "    beta=params[13]\n",
    "    \n",
    "    \n",
    "    global R,X\n",
    "    assert (R.shape[0] == X.shape[0]), \"numbers of rows not match!\"\n",
    "    T = X.shape[0]\n",
    "    dim = X.shape[1]\n",
    "    # R = R.flatten()\n",
    "    # X = X.flatten() # flatten arrays\n",
    "\n",
    "    R1 = np.empty( shape = (0, 1) )\n",
    "    X1 = np.empty( shape = (0, dim) )\n",
    "    for t in np.arange(0, T-tau):\n",
    "        R1 = np.append(R1, R[t+tau].reshape(1, 1), axis = 0)\n",
    "        X1 = np.append(X1, X[t, :].reshape(1, dim), axis = 0)\n",
    "\n",
    "    a = np.mat(R1)\n",
    "    R1=a.T\n",
    "    X1 = np.mat(X1)\n",
    "    mean=np.average(R1)\n",
    "    \n",
    "    # Long_run volatility\n",
    "    long_run=omega/(1-alpha-beta)\n",
    "    \n",
    "    #calculating realised and conditional volatility\n",
    "    resid=R1-mu_0-X1[:,0]*mu_1-X1[:,1]*mu_2-X1[:,2]*mu_3-X1[:,3]*mu_4-X1[:,4]*mu_5-X1[:,5]*mu_6-X1[:,6]*mu_7-X1[:,7]*mu_8-X1[:,8]*mu_9-X1[:,9]*mu_10\n",
    "    \n",
    "    realised=abs(resid)\n",
    "    c=np.array(realised)\n",
    "    conditional=np.zeros(len(R1))\n",
    "    conditional[0]=long_run\n",
    "    for t in range(1,len(R1)):\n",
    "        conditional[t]=omega+alpha*resid[t-1]**2+beta*conditional[t-1]\n",
    "        \n",
    "    #calculating log_likelihood\n",
    "    likelihood=(1/((2*np.pi)**(1/2)*conditional**(1/2)))*np.exp(-c**2/(2*conditional))\n",
    "    log_likelihood=np.sum(np.log(likelihood))\n",
    "    \n",
    "    return -log_likelihood\n",
    "\n",
    "#### Calculate the ML estimates for one month-horizon multivariate regression models\n",
    "def multivar_ML(R, X, tau):\n",
    "    \n",
    "    assert (R.shape[0] == X.shape[0]), \"numbers of rows not match!\"\n",
    "    T = X.shape[0]\n",
    "    dim = X.shape[1]\n",
    "    \n",
    "\n",
    "    R1 = np.empty( shape = (0, 1) )\n",
    "    X1 = np.empty( shape = (0, dim) )\n",
    "    for t in np.arange(0, T-tau):\n",
    "        R1 = np.append(R1, R[t+tau].reshape(1, 1), axis = 0)\n",
    "        X1 = np.append(X1, X[t, :].reshape(1, dim), axis = 0)\n",
    "\n",
    "        \n",
    "    \n",
    "    # Maximizing log_likelihood\n",
    "    params=[0.05,0.6,0.6,0.6,0.6,0.6,0.6,0.01,0.5,0.4,0.5,0.1,0.05,0.92]\n",
    "    res=spop.minimize(garch_mle,params,method=\"Nelder-Mead\",bounds=((-1,1),(-1,1),(-1,1),(-1,1),(-1,1),(-1,1),(-1,1),(-1,1),(-1,1),(-1,1),(-1,1),(0.001,1),(0.001,1),(0.001,1)))\n",
    "    params=res.x\n",
    "    mu_0=res.x[0]\n",
    "    mu=np.mat(res.x[1:11])\n",
    "     \n",
    "        \n",
    "    # compute residuals and residual variance\n",
    "    epsilon = R1 - mu_0 - (X1 @ mu.T)\n",
    "    var_res=np.var(epsilon)\n",
    "    # compute forecast\n",
    "    forecast_tau = mu_0 + ( mu @ X[T-1, :].reshape(dim, 1) )\n",
    "    return forecast_tau,var_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f9e797",
   "metadata": {
    "id": "91f9e797"
   },
   "outputs": [],
   "source": [
    "##### Implement principal component regressors\n",
    "\n",
    "def multivar_lhOLSPC(R, X, num_PCs, tau):\n",
    "    assert (R.shape[0] == X.shape[0]), \"numbers of rows not match!\"\n",
    "    T = X.shape[0]\n",
    "    dim = X.shape[1]\n",
    "    # R = R.flatten()\n",
    "    # X = X.flatten() # flatten arrays\n",
    "    \n",
    "    R1 = np.empty( shape = (0, 1) )\n",
    "    X1 = np.empty( shape = (0, dim) )\n",
    "    for t in np.arange(0, T):\n",
    "        if t < T-tau:\n",
    "            R1 = np.append(R1, R[t+tau].reshape(1, 1), axis = 0)\n",
    "        else:\n",
    "            R1 = np.append(R1, np.array([0]).reshape(1, 1), axis = 0)\n",
    "        X1 = np.append(X1, X[t, :].reshape(1, dim), axis = 0)\n",
    "\n",
    "    # split the numpy array into train and test data\n",
    "    if tau == 0:\n",
    "        X1_train, X1_test = X1[0:(T-1), :], X1[(T-1):, :]\n",
    "        R1_train, R1_test = R1[0:(T-1), :], R1[(T-1):, :]\n",
    "    else:\n",
    "        X1_train, X1_test = X1[0:(T-tau), :], X1[(T-tau):, :]\n",
    "        R1_train, R1_test = R1[0:(T-tau)], R1[(T-tau):]\n",
    "\n",
    "    \n",
    "     # standardizing the features\n",
    "    X11 = StandardScaler().fit_transform(X1_train)\n",
    "    X12= StandardScaler().fit_transform(X1_test)\n",
    "    \n",
    "    pca = PCA(n_components = num_PCs)\n",
    "    X_train = pca.fit_transform(X11)\n",
    "    X_test= pca.transform(X12)\n",
    "    \n",
    "    # do time series regressions\n",
    "    PCs = np.mat(X_train)\n",
    "    PCs1 = sm.add_constant(PCs)\n",
    "    PC=np.mat(X_test)\n",
    "    R1 = np.mat(R1_train)\n",
    "    ts_res = sm.OLS(R1, PCs1).fit()\n",
    "    alpha = ts_res.params[0]\n",
    "    beta = np.mat(ts_res.params[1:])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # compute residuals\n",
    "    epsilon = R1 - alpha - (PCs @ beta.T)\n",
    "    var_res=np.var(epsilon)\n",
    "    \n",
    "    # forecast for the test data\n",
    "    forecast=alpha + (beta@PC.T)\n",
    "    \n",
    "    return forecast,var_res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sraMKEi07Xxj",
   "metadata": {
    "id": "sraMKEi07Xxj"
   },
   "outputs": [],
   "source": [
    "##### Implement Artificial neural network\n",
    "\n",
    "def ANNf(R, X, tau, batch_size: int, num_epochs: int):\n",
    "    assert (R.shape[0] == X.shape[0]), \"numbers of rows not match!\"\n",
    "    T = X.shape[0]\n",
    "    dim = X.shape[1]\n",
    "    # R = R.flatten()\n",
    "    # X = X.flatten() # flatten arrays\n",
    "\n",
    "    R1 = np.empty( shape = (0, 1) )\n",
    "    X1 = np.empty( shape = (0, dim) )\n",
    "    for t in np.arange(0, T):\n",
    "        if t < T-tau:\n",
    "            R1 = np.append(R1, R[t+tau].reshape(1, 1), axis = 0)\n",
    "        else:\n",
    "            R1 = np.append(R1, np.array([0]).reshape(1, 1), axis = 0)\n",
    "        X1 = np.append(X1, X[t, :].reshape(1, dim), axis = 0)\n",
    "\n",
    "    # studentize data\n",
    "    # X1 = StandardScaler().fit_transform(X1)\n",
    "\n",
    "    # split the numpy array into train and test data\n",
    "    if tau == 0:\n",
    "        X1_train, X1_test = X1[0:(T-1), :], X1[(T-1):, :]\n",
    "        R1_train, R1_test = R1[0:(T-1), :], R1[(T-1):, :]\n",
    "    else:\n",
    "        X1_train, X1_test = X1[0:(T-tau), :], X1[(T-tau):, :]\n",
    "        R1_train, R1_test = R1[0:(T-tau)], R1[(T-tau):]\n",
    "\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "\n",
    "    # define the first layer of the ANN\n",
    "    model.add( Dense( 100, input_shape= (dim, ) ) )\n",
    "    model.add(Activation('relu'))\n",
    "    model.add( Dropout(0.5) )\n",
    "\n",
    "    # define the second layer\n",
    "    model.add( Dense(50) )\n",
    "    model.add( Activation('relu') )\n",
    "    model.add( Dropout(0.5) )\n",
    "\n",
    "    # define the third layer\n",
    "    model.add( Dense(10) )\n",
    "    model.add( Activation('relu') )\n",
    "    model.add( Dropout(0.5) )\n",
    "\n",
    "    # define the output layer\n",
    "    model.add( Dense(1) )\n",
    "    model.add( Activation('relu') )\n",
    "\n",
    "\n",
    "    # compile the ANN model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    # model.summary()\n",
    "\n",
    "    # train the model\n",
    "    model.fit(X1_train, R1_train, batch_size=batch_size, shuffle=False, epochs=num_epochs, verbose=0) # Verbosity mode 0: silent\n",
    "\n",
    "    # calculate residuals for the train data\n",
    "    R1_pred = model(X1_train)\n",
    "    # print('R1_pred = \\n', R1_pred)\n",
    "    residuals = R1_train - R1_pred\n",
    "    \n",
    "    var_res=np.var(residuals)\n",
    "\n",
    "\n",
    "    # forecast for the test data\n",
    "    forecasts = model(X1_test)\n",
    "    # print('forecasts = \\n', forecasts)\n",
    "    forecast_tau = float(forecasts[len(forecasts) - 1])\n",
    "    \n",
    "    K.clear_session()\n",
    "    del model # delete the model\n",
    "\n",
    "    # output results\n",
    "    if tau == 0:\n",
    "        return forecast_tau\n",
    "    else:\n",
    "        return forecast_tau,var_res\n",
    "    \n",
    "    \n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LHQfOCb77X9q",
   "metadata": {
    "id": "LHQfOCb77X9q"
   },
   "outputs": [],
   "source": [
    "##### Calculate the mean squared error of XGBoost forecasts for validating samples\n",
    "def rmsfe_XGBoost(args, R_train, X_train, R_valid, X_valid, seed):\n",
    "    \"\"\"   seed               : model seed\n",
    "    booster : booster to use (gbtree, gblinear or dart; gbtree and dart use tree based models while gblinear uses linear functions)\n",
    "    n_estimators       : number of boosted trees to fit\n",
    "    max_depth          : maximum tree depth for base learners\n",
    "    learning_rate      : boosting learning rate (xgb’s “eta”)\n",
    "    max_delta_step : maximum step size\n",
    "    min_child_weight   : minimum sum of instance weight(hessian) needed in a child\n",
    "    subsample          : subsample ratio of the training instance\n",
    "    colsample_bytree   : subsample ratio of columns when constructing each tree\n",
    "    colsample_bylevel  : subsample ratio of columns for each split, in each level\n",
    "    gamma              :  regularization hyperparameter \n",
    "    reg_alpha, reg_lambda: regularization parameters \"\"\"\n",
    "\n",
    "    # global models, train_scores, test_scores, curr_model_hyper_params\n",
    "    curr_model_hyper_params = ['colsample_bylevel', 'colsample_bytree', 'gamma', 'learning_rate', 'max_delta_step', 'max_depth', \\\n",
    "                                                        'min_child_weight', 'n_estimators', 'reg_alpha', 'reg_lambda', 'subsample']\n",
    "    params = {curr_model_hyper_params[i]: args[i] for i, j in enumerate(curr_model_hyper_params)}\n",
    "    model = XGBRegressor(booster='gbtree', objective ='reg:squarederror', random_state=42, seed=seed)\n",
    "    model.set_params(**params)\n",
    "    model.fit(X_train, R_train) # fit training samples to model\n",
    "    R_pred = model.predict(X_valid)\n",
    "    msfe = mean_squared_error(R_valid, R_pred)\n",
    "    del model\n",
    "    return msfe\n",
    "\n",
    "##### Find optimal hyperparameters for XGBoost with cross validation\n",
    "def minimize_XGBoost(R_train, X_train, R_valid, X_valid, seed = 100, n_calls = 50):\n",
    "    # defining the space\n",
    "    space = [\n",
    "        Real(0.1, 1, name=\"colsample_bylevel\"),\n",
    "        Real(0.1, 1, name=\"colsample_bytree\"),\n",
    "        Real(0, 1, name=\"gamma\"),\n",
    "        Real(0, 1, name=\"learning_rate\"),\n",
    "        Real(0, 10, name=\"max_delta_step\"),\n",
    "        Integer(1, 15, name=\"max_depth\"),\n",
    "        Real(0.1, 500, name=\"min_child_weight\"),\n",
    "        Integer(10, 100, name=\"n_estimators\"),\n",
    "        Real(0, 0.5, name=\"reg_alpha\"),\n",
    "        Real(0, 0.5, name=\"reg_lambda\"),\n",
    "        Real(0.1, 1, name=\"subsample\"),\n",
    "    ]\n",
    "\n",
    "    objective_function = partial(rmsfe_XGBoost, R_train=R_train, X_train=X_train, R_valid=R_valid, X_valid=X_valid, seed=seed)\n",
    "\n",
    "    # minimize the RMSFE\n",
    "    res = gp_minimize(objective_function, space, base_estimator=None, n_calls=n_calls, n_random_starts=n_calls-1, random_state=42, n_jobs=1)\n",
    "    return res.x\n",
    "\n",
    "##### Implement XGBoost using cross validation\n",
    "def XGBoostf_CV(R, X, tau: int, seed=1234, n_calls = 150):\n",
    "    assert (R.shape[0] == X.shape[0]), \"numbers of rows not match!\"\n",
    "    assert(tau > 0), \"the forecast horizon must be greater than zero!\"\n",
    "    \n",
    "    T = X.shape[0]\n",
    "    dim = X.shape[1]\n",
    "    # R = R.flatten()\n",
    "    # X = X.flatten() # flatten arrays\n",
    "\n",
    "    R1 = np.empty( shape = (0, 1) )\n",
    "    X1 = np.empty( shape = (0, dim) )\n",
    "    for t in np.arange(0, T):\n",
    "        if t < T-tau:\n",
    "            R1 = np.append(R1, R[t+tau].reshape(1, 1), axis = 0)\n",
    "        else:\n",
    "            R1 = np.append(R1, np.array([0]).reshape(1, 1), axis = 0)\n",
    "        X1 = np.append(X1, X[t, :].reshape(1, dim), axis = 0)\n",
    "\n",
    "    # studentize data\n",
    "    X1 = StandardScaler().fit_transform(X1)\n",
    "\n",
    "    # split the numpy array into train, validation, and test data\n",
    "    sratio = 0.8\n",
    "    X1_train, X1_valid, X1_test = X1[0:(math.floor(sratio*T)-tau), :].reshape(-1, dim), X1[(math.floor(sratio*T)-tau):(T-tau), :].reshape(-1, dim), \\\n",
    "                                                                                                                                                                                X1[(T-tau):, :].reshape(-1, dim)\n",
    "    R1_train, R1_valid, R1_test = R1[0:(math.floor(sratio*T)-tau), :].reshape(-1, 1), R1[(math.floor(sratio*T)-tau):(T-tau), :].reshape(-1, 1), \\\n",
    "                                                                                                                                                                                    R1[(T-tau):, :].reshape(-1, 1)\n",
    "\n",
    "    # find optimal hyperparameters for XGBoost using cross validation\n",
    "    x = minimize_XGBoost(R1_train, X1_train, R1_valid, X1_valid, seed = seed, n_calls = n_calls)\n",
    "\n",
    "    # define model with optimal hyperparameters\n",
    "    model = XGBRegressor(booster='gbtree', objective ='reg:squarederror', seed=seed, colsample_bylevel=x[0], colsample_bytree=x[1], gamma=x[2], \\\n",
    "                                            learning_rate=x[3], max_delta_step=x[4], max_depth=x[5], min_child_weight=x[6], n_estimators=x[7], \\\n",
    "                                            reg_alpha=x[8], reg_lambda=x[9], subsample=x[10])\n",
    "\n",
    "    # train the model\n",
    "    X1_train_valid = np.concatenate( (X1_train, X1_valid), axis=0)\n",
    "    R1_train_valid = np.concatenate( (R1_train, R1_valid), axis=0)\n",
    "    model.fit(X1_train_valid, R1_train_valid)\n",
    "\n",
    "    # calculate residuals for the train data\n",
    "    R1_pred = model.predict(X1_train_valid)\n",
    "    residuals = R1_train_valid - R1_pred\n",
    "    resid=residuals[0]\n",
    "   \n",
    "    var_res=np.var(resid)\n",
    "\n",
    "\n",
    "    # forecast for the test data\n",
    "    forecasts = model.predict(X1_test)\n",
    "    # print('forecasts = \\n', forecasts)\n",
    "    forecast_tau = float(forecasts[len(forecasts) - 1])\n",
    "    \n",
    "    del model # delete the model\n",
    "\n",
    "    # output results\n",
    "    return forecast_tau, var_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d22b9c",
   "metadata": {
    "id": "f9d22b9c"
   },
   "outputs": [],
   "source": [
    "##### Implement Random Forest\n",
    "\n",
    "# Create the parameter grid based on the results of random search\n",
    "\n",
    "param_grid={'bootstrap':[True],'max_depth':[80,90,100],'max_features':[3,4],'min_samples_leaf':[3,4],'min_samples_split':[8,10],'n_estimators':[200,300]}\n",
    "\n",
    "#Create a based model\n",
    "rf=RandomForestRegressor(random_state=42)\n",
    "#Instatiate the grid search model\n",
    "grid_search=GridSearchCV(estimator=rf,param_grid=param_grid,cv=3,n_jobs=-1,verbose=2)\n",
    "\n",
    "\n",
    "\n",
    "def randomforestf(R, X, tau: int):\n",
    "    assert (R.shape[0] == X.shape[0]), \"numbers of rows not match!\"\n",
    "    assert(tau > 0), \"the forecast horizon must be greater than zero!\"\n",
    "    \n",
    "    T = X.shape[0]\n",
    "    dim = X.shape[1]\n",
    "    # R = R.flatten()\n",
    "    # X = X.flatten() # flatten arrays\n",
    "\n",
    "    R1 = np.empty( shape = (0, 1) )\n",
    "    X1 = np.empty( shape = (0, dim) )\n",
    "    for t in np.arange(0, T):\n",
    "        if t < T-tau:\n",
    "            R1 = np.append(R1, R[t+tau].reshape(1, 1), axis = 0)\n",
    "        else:\n",
    "            R1 = np.append(R1, np.array([0]).reshape(1, 1), axis = 0)\n",
    "        X1 = np.append(X1, X[t, :].reshape(1, dim), axis = 0)\n",
    "\n",
    "    # studentize data\n",
    "    X1 = StandardScaler().fit_transform(X1)\n",
    "\n",
    "    # split the numpy array into train and test data\n",
    "    X1_train, X1_test = X1[0:(T-tau), :], X1[(T-tau):, :]\n",
    "    R1_train, R1_test = R1[0:(T-tau)].ravel(), R1[(T-tau):].ravel()\n",
    "\n",
    "    # create the model with 200 trees\n",
    "    #model = RandomForestRegressor(n_estimators=200, criterion='mse', max_depth=80, min_samples_split=8, min_samples_leaf=3, \\\n",
    "                                                            #max_features=3,  \\\n",
    "                                                             #bootstrap=True, n_jobs=-1, random_state=42, verbose=2)\n",
    "                                                          \n",
    "\n",
    "    # Find optimal hyperparameters\n",
    "    grid_search.fit(X1_train,R1_train)\n",
    "    # Random forest model\n",
    "    best_grid=grid_search.best_estimator_\n",
    "    model=best_grid\n",
    "    \n",
    "    # train the model\n",
    "    model.fit(X1_train, R1_train)\n",
    "\n",
    "    # calculate residuals for the train data\n",
    "    R1_pred = model.predict(X1_train)\n",
    "    residuals = R1_train - R1_pred\n",
    "    \n",
    "    var_res=np.var(residuals)\n",
    "\n",
    "\n",
    "    # forecast for the test data\n",
    "    forecasts = model.predict( X1_test.reshape(-1, dim) )\n",
    "    # print('forecasts = \\n', forecasts)\n",
    "    forecast_tau = float(forecasts[len(forecasts) - 1])\n",
    "    \n",
    "    del model # delete the model\n",
    "\n",
    "    # output results\n",
    "    return forecast_tau, var_res\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274ac1cb",
   "metadata": {
    "id": "274ac1cb"
   },
   "outputs": [],
   "source": [
    "###### Calculate rolling window Ouf-of-sample R_sq for the Garch(1,1) model\n",
    "\n",
    "def OoS_R_sq(sstart: int, df: pd.DataFrame, tau, wsize: int, T1: int, ylag: int, use_model)-> tuple:\n",
    "    \"\"\" 'df': a pandas dataframe, 'freq': a frequency of type 'string', e.g., '3M', '1D', '1Y', or '1Q' etc., 'tau': forecast horizon, 'wsize': window size,\n",
    "            'sstart': sub-sample starting point, 'T1': size of a sub-sample, 'ylag': AR lag of the dependent variable \"\"\"\n",
    "    assert (T1 > wsize+tau), \"size of a subsample must be much greater than the window size!\"\n",
    "    assert (sstart+T1 <= len(df) ), \"end point of the last subsample must be less than or equal to size of the dataframe!\"\n",
    "\n",
    "    df = df.iloc[sstart:(sstart+T1), :].copy()\n",
    "    # get data for the dependant variable and predictors\n",
    "    if ylag > 0:\n",
    "        for i in np.arange(1, ylag+1):\n",
    "            df[f'ylag{i}'] = df.iloc[:, 1].shift(i)\n",
    "        df.dropna(inplace = True)\n",
    "    # print( df.iloc[0, 0] )\n",
    "\n",
    "    R = np.array(df.values[:, 1], dtype='float64')\n",
    "    X = np.array(df.values[:, 2:], dtype='float64')\n",
    "    \n",
    "    dim = X.shape[1]\n",
    "    \n",
    "\n",
    "    rmse = 0\n",
    "    var = 0\n",
    "    mae = 0\n",
    "    #list_err = []\n",
    "    list_forecast=[]\n",
    "    list_rol_var_res=[]\n",
    "    observe_data=[]\n",
    "    list_rol_var=[]\n",
    "    list_rolavg=[]\n",
    "    for s in np.arange(T1-wsize-tau-ylag):\n",
    "        if use_model=='multivar_ML':\n",
    "            forecast,var_res= multivar_ML(R[s:(s+wsize+1)].reshape(-1,1), X[s:(s+wsize+1), :].reshape(-1,dim),tau)\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            print(f'Model {use_model} does not exist!')\n",
    "            sys.exit()   \n",
    "        \n",
    "        \n",
    "        a=np.mean(R[s:(s+wsize)])\n",
    "        list_forecast.append(forecast)\n",
    "        list_rol_var_res.append(var_res)\n",
    "        observe_data.append(R[(s+wsize+1):(s+wsize+2)])\n",
    "        r = R[s+wsize+tau] # actual returns\n",
    "        var1= np.var(R[s:(s+wsize)])\n",
    "        list_rol_var.append(var1)\n",
    "        list_rolavg.append(a)\n",
    "        rmse += pow(r - forecast, 2) / (T1-wsize-tau)\n",
    "        var += pow(r- a , 2) / (T1-wsize-tau)\n",
    "        #mae += abs(r - forecast) / (T1-wsize-tau)\n",
    "    del df # delete this copy of the dataframe\n",
    "    return  float(1 - rmse/(var + 0.00001)),list_forecast,list_rol_var_res,observe_data,list_rol_var,list_rolavg\n",
    "   \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3271aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file=pd.read_csv(all_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee9c06d",
   "metadata": {
    "id": "4ee9c06d"
   },
   "outputs": [],
   "source": [
    "#b=OoS_R_sq(sstart=0, df=file, tau=1, wsize=85, T1=167, ylag=0, use_model='multivar_ML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c5e802",
   "metadata": {
    "id": "76c5e802"
   },
   "outputs": [],
   "source": [
    "a1=np.array(b[1])\n",
    "c1=np.mat(a1)\n",
    "d1=pd.DataFrame(c1)\n",
    "d1=d1.T\n",
    "d1=d1.fillna(0)\n",
    "\n",
    "a2=np.array(b[2])\n",
    "c2=np.mat(a2)\n",
    "d2=pd.DataFrame(c2)\n",
    "d2=d2.T\n",
    "d2=d2.fillna(0)\n",
    "\n",
    "a3=np.array(b[3])\n",
    "c3=np.mat(a3)\n",
    "d3=pd.DataFrame(c3)\n",
    "d3=d3.fillna(0)\n",
    "\n",
    "d4=pd.DataFrame(b[4])\n",
    "d4=d4.fillna(0)\n",
    "\n",
    "q= file['excess_log_return'].ewm(com=0.40).mean()\n",
    "q=pd.DataFrame(q)\n",
    "q=q.iloc[86:,]\n",
    "q=q.values\n",
    "q=pd.DataFrame(q)\n",
    "\n",
    "a5=np.array(b[5])\n",
    "c5=np.mat(a5)\n",
    "d5=pd.DataFrame(c5)\n",
    "d5=d5.T\n",
    "d5=d5.fillna(0)\n",
    "\n",
    "df=pd.concat([d1,d2,d3,d4,d5,q],axis=1)\n",
    "df.columns=['return_forecast','rol_var_res','observe_return','rolling_var','rol_avg','ewma']\n",
    "df.to_csv(r'C:/Users/endou012/emile/sDOC_result/file_63.csv' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3591f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Calculate rolling-window OoS R^2 for 'one'-steps ahead forecasts\n",
    "\n",
    "def OoS_R_sq(sstart: int, df: pd.DataFrame, tau, wsize: int, T1: int, ylag: int,batch_size:int,num_epochs:int, use_model)-> tuple:\n",
    "    \"\"\" 'df': a pandas dataframe, 'freq': a frequency of type 'string', e.g., '3M', '1D', '1Y', or '1Q' etc., 'tau': forecast horizon, 'wsize': window size,\n",
    "            'sstart': sub-sample starting point, 'T1': size of a sub-sample, 'ylag': AR lag of the dependent variable \"\"\"\n",
    "    assert (T1 > wsize+tau), \"size of a subsample must be much greater than the window size!\"\n",
    "    assert (sstart+T1 <= len(df) ), \"end point of the last subsample must be less than or equal to size of the dataframe!\"\n",
    "\n",
    "    df = df.iloc[sstart:(sstart+T1), :].copy()\n",
    "    # get data for the dependant variable and predictors\n",
    "    if ylag > 0:\n",
    "        for i in np.arange(1, ylag+1):\n",
    "            df[f'ylag{i}'] = df.iloc[:, 1].shift(i)\n",
    "        df.dropna(inplace = True)\n",
    "    # print( df.iloc[0, 0] )\n",
    "\n",
    "    R = np.array(df.values[:, 1], dtype='float64')\n",
    "    X = np.array(df.values[:, 2:], dtype='float64')\n",
    "    \n",
    "    dim = X.shape[1]\n",
    "    global forecast\n",
    "    \n",
    "    rmse = 0\n",
    "    var = 0\n",
    "    #mae = 0\n",
    "    list_forecast=[]\n",
    "    list_rol_var_res=[]\n",
    "    observe_data=[]\n",
    "    list_rol_var=[]\n",
    "    list_rolavg=[]\n",
    "    #list_err = []\n",
    "    for s in np.arange(T1-wsize-tau-ylag):\n",
    "    # estimate a long-run regression model and make a 'tau'-steps ahead forecast\n",
    "        \n",
    "        if use_model == 'multivar_lhOLSPC':\n",
    "            r_forecast, var_res= multivar_lhOLSPC(R[s:(s+wsize+1)].reshape(-1, 1), X[s:(s+wsize+1), :].reshape(-1, dim), num_PCs, tau)                                                                                                 num_PCs, tau)\n",
    "        \n",
    "        elif use_model == 'ANNf':\n",
    "            r_forecast, var_res = ANNf(R[s:(s+wsize+1)].reshape(-1, 1), X[s:(s+wsize+1), :].reshape(-1, dim), tau, batch_size, num_epochs)                                                                                                        batch_size, num_epochs)\n",
    "        \n",
    "        elif use_model == 'xgboostf_cv':\n",
    "            r_forecast, var_res = XGBoostf_CV(R[s:(s+wsize+1)].reshape(-1, 1), X[s:(s+wsize+1), :].reshape(-1, dim), tau,  n_calls=50)                                                                                                                n_calls=50)\n",
    "        elif use_model == 'randomforestf':\n",
    "            r_forecast, var_res= randomforestf(R[s:(s+wsize+1)].reshape(-1, 1), X[s:(s+wsize+1), :].reshape(-1, dim), tau)\n",
    "        else:\n",
    "            print(f'Model {use_model} does not exist!')\n",
    "            sys.exit()\n",
    "\n",
    "        a=np.mean(R[s:(s+wsize)])\n",
    "        list_forecast.append(r_forecast)\n",
    "        list_rol_var_res.append(var_res)\n",
    "        observe_data.append(R[(s+wsize+1):(s+wsize+2)])\n",
    "        \n",
    "        r = R[s+wsize+tau] # actual returns\n",
    "        var1=np.var(R[s:(s+wsize)])\n",
    "        list_rol_var.append(var1)\n",
    "        list_rolavg.append(a)\n",
    "\n",
    "        rmse +=  pow(r - r_forecast, 2) / (T1-wsize-tau)\n",
    "        var += pow(r - a, 2) / (T1-wsize-tau)\n",
    "    \n",
    "    \n",
    "    del df # delete this copy of the dataframe\n",
    "    return  float(1 - rmse/(var + 0.00001) ),list_forecast,list_rol_var_res,observe_data,list_rol_var,list_rolavg\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "b196ba9c",
   "metadata": {
    "id": "b196ba9c"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afe9168",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fil=pd.read_csv(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ca0692",
   "metadata": {
    "id": "52ca0692"
   },
   "outputs": [],
   "source": [
    "\n",
    "#b=OoS_R_sq(sstart=0,df=fil,tau=1,wsize=150,T1=285,batch_size=256,num_epochs=150,ylag=1,use_model='ANNf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a20d8f2",
   "metadata": {
    "id": "4a20d8f2"
   },
   "outputs": [],
   "source": [
    "a1=np.array(b[1])\n",
    "c1=np.mat(a1)\n",
    "d1=pd.DataFrame(c1)\n",
    "d1=d1.T\n",
    "d1=d1.fillna(0)\n",
    "\n",
    "a2=np.array(b[2])\n",
    "c2=np.mat(a2)\n",
    "d2=pd.DataFrame(c2)\n",
    "d2=d2.T\n",
    "d2=d2.fillna(0)\n",
    "\n",
    "a3=np.array(b[3])\n",
    "c3=np.mat(a3)\n",
    "d3=pd.DataFrame(c3)\n",
    "d3=d3.fillna(0)\n",
    "\n",
    "d4=pd.DataFrame(b[4])\n",
    "d4=d4.fillna(0)\n",
    "\n",
    "a5=np.array(b[5])\n",
    "c5=np.mat(a5)\n",
    "d5=pd.DataFrame(c5)\n",
    "d5=d5.T\n",
    "d5=d5.fillna(0)\n",
    "\n",
    "q=fil['excess_log_return'].ewm(com=0.40).mean()\n",
    "q=pd.DataFrame(q)\n",
    "q=q.iloc[151:]\n",
    "q=q.values\n",
    "q=pd.DataFrame(q)\n",
    "\n",
    "df=pd.concat([d1,d2,d3,d4,d5,q],axis=1)\n",
    "df.columns=['return_forecast','rol_var_res','observe_return','rolling_var','rol_avg','ewm']\n",
    "df.to_csv('/content/gdrive/MyDrive/resultNeural/file_36.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f449d07e",
   "metadata": {
    "id": "f449d07e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46668eff",
   "metadata": {
    "id": "46668eff"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c791eb",
   "metadata": {
    "id": "f2c791eb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdcc6fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1646687364792,
     "user": {
      "displayName": "NDOUMBE Emile Herve",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09643065439420591964"
     },
     "user_tz": 300
    },
    "id": "5bdcc6fe",
    "outputId": "61cfff9f-81ce-4d38-c44f-8e5c2139ba05"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a878aa2",
   "metadata": {
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1648248949102,
     "user": {
      "displayName": "NDOUMBE Emile Herve",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09643065439420591964"
     },
     "user_tz": 240
    },
    "id": "6a878aa2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36726e3",
   "metadata": {
    "id": "d36726e3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b0cda3",
   "metadata": {
    "id": "83b0cda3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34231b14",
   "metadata": {
    "id": "34231b14"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "675779dc",
   "metadata": {
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1648248009410,
     "user": {
      "displayName": "NDOUMBE Emile Herve",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09643065439420591964"
     },
     "user_tz": 240
    },
    "id": "675779dc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4487893c",
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1648247314055,
     "user": {
      "displayName": "NDOUMBE Emile Herve",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09643065439420591964"
     },
     "user_tz": 240
    },
    "id": "4487893c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55cc8da9",
   "metadata": {
    "executionInfo": {
     "elapsed": 139,
     "status": "ok",
     "timestamp": 1648247981728,
     "user": {
      "displayName": "NDOUMBE Emile Herve",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09643065439420591964"
     },
     "user_tz": 240
    },
    "id": "55cc8da9"
   },
   "outputs": [],
   "source": [
    "\n",
    "         \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f61a9d",
   "metadata": {
    "id": "e5f61a9d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f11587a",
   "metadata": {
    "id": "9f11587a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729daf7",
   "metadata": {
    "id": "1729daf7"
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320aa72f",
   "metadata": {
    "id": "320aa72f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b16ec7",
   "metadata": {
    "id": "e6b16ec7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07b15d4",
   "metadata": {
    "id": "c07b15d4"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc27fea7",
   "metadata": {
    "id": "dc27fea7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1929d4c",
   "metadata": {
    "id": "b1929d4c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be116b",
   "metadata": {
    "id": "22be116b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c412dd4b",
   "metadata": {
    "id": "c412dd4b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22eec067",
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1648249034938,
     "user": {
      "displayName": "NDOUMBE Emile Herve",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09643065439420591964"
     },
     "user_tz": 240
    },
    "id": "22eec067"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864a17bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 495,
     "status": "ok",
     "timestamp": 1648249921955,
     "user": {
      "displayName": "NDOUMBE Emile Herve",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09643065439420591964"
     },
     "user_tz": 240
    },
    "id": "864a17bd",
    "outputId": "08381023-91ae-46fe-f38c-f7dcc8fbb6a1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZAp7xUPgj7Ji",
   "metadata": {
    "id": "ZAp7xUPgj7Ji"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "wcWYWsyKnQ_S",
   "metadata": {
    "executionInfo": {
     "elapsed": 127727,
     "status": "ok",
     "timestamp": 1648250377920,
     "user": {
      "displayName": "NDOUMBE Emile Herve",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09643065439420591964"
     },
     "user_tz": 240
    },
    "id": "wcWYWsyKnQ_S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8bab1d45",
   "metadata": {
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1648250431028,
     "user": {
      "displayName": "NDOUMBE Emile Herve",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09643065439420591964"
     },
     "user_tz": 240
    },
    "id": "8bab1d45"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7-eWFZnv_HbY",
   "metadata": {
    "id": "7-eWFZnv_HbY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aGk9lm4i-6jI",
   "metadata": {
    "id": "aGk9lm4i-6jI"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d87f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1648250411664,
     "user": {
      "displayName": "NDOUMBE Emile Herve",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09643065439420591964"
     },
     "user_tz": 240
    },
    "id": "a49d87f7",
    "outputId": "1451351a-f8cb-4325-c594-605f0ec620af"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6dfc95",
   "metadata": {
    "id": "8b6dfc95"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59a97d9",
   "metadata": {
    "id": "a59a97d9"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "forecast_est_monthly.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
