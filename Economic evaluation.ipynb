{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dd4b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================= Economic evaluation of machine learning methods, graphical representation of excess return of machine learning portfolio, grahical represention of DOC-GARCH(1,1) features=====================================#\n",
    "#=============================================================================================================================================================================================================================================#\n",
    "\n",
    "\n",
    "!pip install dask\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install \"plotly>=4\" numpy pandas\n",
    "!pip install -U kaleido\n",
    "!pip install pyfolio\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import dask.dataframe as dd\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from timeit import default_timer as timer\n",
    "import warnings\n",
    "from pandas import plotting\n",
    "from typing import Tuple\n",
    "import pyfolio as pf\n",
    "import math\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"images\"):\n",
    "    os.mkdir(\"images\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25202591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ouf-of-sample R^2 results based on machine learning methods\n",
    "\n",
    "file=pd.read_csv(r'C:/Users/Emile Ndoumbe/Dropbox/Oo_R_sq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03c919f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = r'C:/Users/Emile Ndoumbe/Dropbox/dossier_machinelearning_data/sDOC_results'                     \n",
    "all_files1 = glob.glob(path1 + \"/*.csv\")\n",
    "\n",
    "path2 = r'C:/Users/Emile Ndoumbe/Dropbox/dossier_machinelearning_data/PCRresult'                     \n",
    "all_files2 = glob.glob(path2 + \"/*.csv\")\n",
    "\n",
    "\n",
    "path3 = r'C:/Users/Emile Ndoumbe/Dropbox/dossier_machinelearning_data/Rdomfo_results'                     \n",
    "all_files3 = glob.glob(path3 + \"/*.csv\")\n",
    "\n",
    "path5 = r'C:/Users/Emile Ndoumbe/Dropbox/dossier_machinelearning_data/NNfresult'                     \n",
    "all_files5 = glob.glob(path5 + \"/*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecd0f02",
   "metadata": {},
   "source": [
    "# Gain in terms of sharpe ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cca8c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gain in terms of sharpe ratio for the sDOC method\n",
    "\n",
    "# Set the current working directory\n",
    "\n",
    "path1 = r'C:/Users/Emile Ndoumbe/Dropbox/dossier_machinelearning_data/sDOC_results'                     \n",
    "all_files1 = glob.glob(path1 + \"/*.csv\")\n",
    "\n",
    "# Gain in terms of sharpe ratio\n",
    "sharpe_ratio_gain1=[]\n",
    "for i in range(63):\n",
    "    fil=pd.read_csv(all_files1[i])\n",
    "    sharpe=[]\n",
    "    sharp=[]\n",
    "    for j in range(fil.shape[0]):\n",
    "        SR_0=(fil.iloc[j,6])/math.sqrt(fil.iloc[j,4])\n",
    "        SR=(fil.iloc[j,1])/math.sqrt(fil.iloc[j,2])\n",
    "        sharpe.append(SR_0)\n",
    "        sharp.append(SR)\n",
    "    sharpe_1=np.mean(sharpe)\n",
    "    shp=np.mean(sharp)\n",
    "    \n",
    "    #sharpe_2=(sharpe_1+file.iloc[i,3])/(1-file.iloc[i,3])\n",
    "    #SR_1=math.sqrt(sharpe_2)- shp\n",
    "    SR_1=shp-sharpe_1\n",
    "    \n",
    "    sharpe_ratio_gain1.append(SR_1)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "2c3f9fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gain in terms of sharpe ratio for the PCA method\n",
    "\n",
    "# Set the current working directory\n",
    "\n",
    "path2 = r'C:/Users/Emile Ndoumbe/Dropbox/dossier_machinelearning_data/PCRresult'                     \n",
    "all_files2 = glob.glob(path2 + \"/*.csv\")\n",
    "\n",
    "# Gain in terms of sharpe ratio\n",
    "\n",
    "sharpe_ratio_gain2=[]\n",
    "for i in range(63):\n",
    "    fil=pd.read_csv(all_files2[i])\n",
    "    sharpe=[]\n",
    "    sharp=[]\n",
    "    for j in range(fil.shape[0]):\n",
    "        SR_0=(fil.iloc[j,6]**2)/fil.iloc[j,4]\n",
    "        SR=(fil.iloc[j,6])/math.sqrt(fil.iloc[j,4])\n",
    "        sharpe.append(SR_0)\n",
    "        sharp.append(SR)\n",
    "    sharpe_1=np.mean(sharpe)\n",
    "    shp=np.mean(sharp)\n",
    "    \n",
    "    sharpe_2=(sharpe_1+file.iloc[i,3])/(1-file.iloc[i,3])\n",
    "    SR_1=math.sqrt(sharpe_2)- shp\n",
    "    sharpe_ratio_gain2.append(SR_1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "1c2925d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gain in terms of sharpe ratio for the random forest method\n",
    "\n",
    "# Set the current working directory\n",
    "\n",
    "path3 = r'C:/Users/Emile Ndoumbe/Dropbox/dossier_machinelearning_data/Rdomfo_results'                     \n",
    "all_files3 = glob.glob(path3 + \"/*.csv\")\n",
    "\n",
    "sharpe_ratio_gain3=[]\n",
    "for i in range(63):\n",
    "    fil=pd.read_csv(all_files3[i])\n",
    "    sharpe=[]\n",
    "    sharp=[]\n",
    "    for j in range(fil.shape[0]):\n",
    "        SR_0=(fil.iloc[j,6]**2)/fil.iloc[j,4]\n",
    "        SR=(fil.iloc[j,6])/math.sqrt(fil.iloc[j,4])\n",
    "        sharpe.append(SR_0)\n",
    "        sharp.append(SR)\n",
    "    sharpe_1=np.mean(sharpe)\n",
    "    shp=np.mean(sharp)\n",
    "    \n",
    "    sharpe_2=(sharpe_1+file.iloc[i,3])/(1-file.iloc[i,3])\n",
    "    SR_1=math.sqrt(sharpe_2)- shp\n",
    "    sharpe_ratio_gain3.append(SR_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96da1143",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gain in terms of sharpe ratio for the neural network method\n",
    "\n",
    "# Set the current working directory\n",
    "\n",
    "path5 = r'C:/Users/Emile Ndoumbe/Dropbox/dossier_machinelearning_data/NNfresult'                     \n",
    "all_files5 = glob.glob(path5 + \"/*.csv\")\n",
    "\n",
    "# Gain in terms of sharpe ratio\n",
    "sharpe_ratio_gain5=[]\n",
    "for i in range(63):\n",
    "    fil=pd.read_csv(all_files5[i])\n",
    "    sharpe=[]\n",
    "    for j in range(fil.shape[0]):\n",
    "        SR_0=(fil.iloc[j,6]**2)/fil.iloc[j,4]\n",
    "        sharpe.append(SR_0)\n",
    "    sharpe_1=np.mean(sharpe)\n",
    "    sharpe_2=(sharpe_1+file.iloc[i,4])/(1-file.iloc[i,4])\n",
    "    SR_1=math.sqrt(sharpe_2)- math.sqrt(sharpe_1) \n",
    "    \n",
    "    sharpe_ratio_gain5.append(SR_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd1093",
   "metadata": {},
   "source": [
    "# Gain in terms of average excess portfolio return, average utility, transaction cost, turnover and maximum drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2359,
   "id": "87af10fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that performs a division: if the denominator is zero, the quotient of the division is fixed to zero\n",
    "def safe_div(x,y):\n",
    "    if y == 0:\n",
    "        return 0\n",
    "    return x / y\n",
    "\n",
    "## Excess portfolio return series for machine learning method, excess portfolio return for a buy-and-hold investor,\n",
    "## Utility values for machine learning method, utilility series for a buy-and-hold investor, monthly turnover series,\n",
    "\n",
    "vector_1=[]\n",
    "vector_2=[]\n",
    "vector_3=[]\n",
    "vector_4=[]\n",
    "vector_5=[]\n",
    "vector_6=[]\n",
    "\n",
    "fil=pd.read_csv(all_files1[0])\n",
    "fil=fil.fillna(0)\n",
    "a=fil.shape[0]\n",
    "\n",
    "# Investor with a small relative risk aversion\n",
    "gamma=3\n",
    "\n",
    "for i in range(a):\n",
    "    x=fil.iloc[i,1] # vector of out-of-sample forecasts \n",
    "    y=fil.iloc[i,2] # vector of one-period ahead predicted volatility\n",
    "    f=fil.iloc[i,5] # vector of rolling average of excess industry-level return\n",
    "    g=fil.iloc[i,4] # vector of rolling variance of excess industry-level return\n",
    "    l=fil.iloc[i,3] # vector of observed excess industry-level returns \n",
    "    e=safe_div(x,y) # vector of out-of-sample forecasts divided by one-period ahead predicted volatility\n",
    "    \n",
    "    # Optimal portfolio weight\n",
    "    w=e/gamma \n",
    "    \n",
    "    # Utility corresponding to a machine learning method (after simplification)\n",
    "    u_1=(w*x)/2 \n",
    "    \n",
    "    # Excess portfolio return using a machine learning method\n",
    "    \n",
    "    u_2=w*l \n",
    "    \n",
    "    # vector of rolling average of excess industry-level return divided by the rolling variance of excess industry-level return\n",
    "        \n",
    "    h=safe_div(f,g)\n",
    "    \n",
    "    # Vector of weight corresponding to a buy-and-hold strategy\n",
    "    v=h/gamma\n",
    "    \n",
    "    # Utility corresponding to a buy-and-hold investor (after simplification)\n",
    "    m_1=(v*f)/2   \n",
    "    \n",
    "    # Excess portfolio return for a buy-and-hold investor\n",
    "    \n",
    "    m_2=v*l \n",
    "    vector_1.append(w)\n",
    "    vector_2.append(v)\n",
    "    vector_3.append(u_1)\n",
    "    vector_4.append(u_2)\n",
    "    vector_5.append(m_1)\n",
    "    vector_6.append(m_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2360,
   "id": "954218c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average utility for a machine learning method\n",
    "dd=sum(vector_3)/len(vector_3) \n",
    "\n",
    "# Average utility for a buy-and-hold investor\n",
    "bb=sum(vector_5)/len(vector_5)\n",
    "\n",
    "# Average portfolio return for a machine learning method\n",
    "cc=sum(vector_4)/len(vector_4)\n",
    "\n",
    "#Average portfolio return for a buy-and-hold investor\n",
    "ee=sum(vector_6)/len(vector_6)\n",
    "\n",
    "# Series of excess portfolio return\n",
    "D=pd.Series(vector_4)\n",
    "\n",
    "# Vector of montly turnover\n",
    "I=pd.DataFrame(vector_1)\n",
    "e=abs(I-I.shift(1)) \n",
    "\n",
    "# Average momthly turnover\n",
    "X= np.mean(e) \n",
    "\n",
    "# Transaction cost\n",
    "\n",
    "X_1= dd-bb \n",
    "\n",
    "# Gain in term of average portfolio return\n",
    "X_2= cc-ee\n",
    "\n",
    "DD=pd.DataFrame(vector_4)\n",
    "DD.columns=['raw_ret']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bfffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Maximum drawdom\n",
    "\n",
    "# Series of excess portfolio return associated with date\n",
    "\n",
    "date=pd.period_range('2013-3-29', periods=93, freq='M')\n",
    "data = pd.Series(D.values, index=date)\n",
    "\n",
    "# Maximum drawdown values\n",
    "\n",
    "pf.create_returns_tear_sheet(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a718a1",
   "metadata": {},
   "source": [
    "# Gain to pain ratio, Calmar ratio and Sortino ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2333,
   "id": "aaaeb250",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def annualized_return(df: pd.DataFrame, ret_column = 'raw_ret'):\n",
    "    '''Compute Annualized Returns\n",
    "    INPUT \n",
    "        df: a dataframe containing a column of raw returns\n",
    "    OUTPUT\n",
    "        a float number of annualized return\n",
    "    Reference: https://www.investopedia.com/terms/a/annualized-total-return.asp#:~:text=An%20annualized%20total%20return%20is,the%20annual%20return%20was%20compounded.\n",
    "                        and https://www.assetmacro.com/financial-terms/annualized-return/#:~:text=Annualized%20Return%20is%20the%20average,252%20trading%20days%20of%20year).\n",
    "    '''\n",
    "    gross_return = (1 + df[ret_column]).prod()\n",
    "    months = df.shape[0]\n",
    "    years = months / 12 # there are about 12 months per year \n",
    "    ann_return = gross_return **(1/years)\n",
    "    ann_return = ann_return - 1\n",
    "    return ann_return\n",
    "\n",
    "\n",
    "\n",
    "def gain_to_pain_ratio(df: pd.DataFrame, ret_column = 'raw_ret'):\n",
    "    ''' Calculate Schwager's Gain to Pain Ratio\n",
    "    INPUT \n",
    "        df: a dataframe containing a column of raw returns\n",
    "    OUTPUT\n",
    "        a float number of the gain-to-pain ratio (a Gain to Pain ratio above 1.25 is considered good, and a value over 2 is very good.)\n",
    "    Reference: https://www.investmacro.com/forex/2018/07/5-statistics-for-analyzing-your-trading-performance \n",
    "                        and https://jackschwager.com/market-wizards-search-part-2-the-performance-statistics-i-use '''\n",
    "\n",
    "    net_return = df[ret_column].sum()\n",
    "    abs_negative_return = abs( df[ret_column][df[ret_column] < 0].sum() )\n",
    "    gain_to_pain = net_return / (abs_negative_return + 1e-4)\n",
    "    return gain_to_pain\n",
    "\n",
    "def calmar_ratio(df: pd.DataFrame, ret_column = 'raw_ret'):\n",
    "    '''Annualized Return over Max Drawdown\n",
    "    INPUT \n",
    "        df: a dataframe containing a column of raw returns\n",
    "    OUTPUT\n",
    "        a float number of the Calmar ratio (a Calmar ratio of 0.50 is minimal. Anything over 1.0 is considered a pretty healthy risk adjusted return.)\n",
    "    Reference: Terry W Young. Calmar ratio: A smoother tool. Futures, 20(1):40, 1991. '''\n",
    "\n",
    "    calmar = annualized_return(df) / (max_drawdown + 1e-4)\n",
    "    return calmar\n",
    "\n",
    "def sortino_ratio(df: pd.DataFrame, ret_column = 'raw_ret'):\n",
    "    '''Annualized Return - RF rate / Annualized Downside Standard Deviation\n",
    "    INPUT \n",
    "        df: a dataframe containing a column of raw returns and a column of risk-free rates\n",
    "    OUTPUT\n",
    "        a float number of the Sortino ratio\n",
    "    Reference: https://www.investmacro.com/forex/2018/07/5-statistics-for-analyzing-your-trading-performance  '''\n",
    "\n",
    "    # Calculate the annualized average daily excess return\n",
    "    excess_returns = df[ret_column]\n",
    "    ann_mean = excess_returns.mean() * 12  # there are about 12 months per year\n",
    "\n",
    "    # Calculate the annualized daily standard deviation of negative excess returns\n",
    "    downside_excess_returns = excess_returns[excess_returns < 0]\n",
    "    ann_downside_std_dev = downside_excess_returns.std() * (12 ** 0.5)\n",
    "   \n",
    "    return ann_mean / ann_downside_std_dev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed1d156",
   "metadata": {},
   "source": [
    "# Cumulative excess return of machine learning portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a421368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative excess portfolio return : for each method, compute a cumulative excess return cum_sdoc, cum_pcr, cum_RF, cum_NN3\n",
    "\n",
    "cum_sdoc=D.cumsum() # We can compute the rest of cumulative excess return by changing the method\n",
    "\n",
    "\n",
    "a_0=pd.DataFrame(fil.iloc[:,0])\n",
    "\n",
    "# Put the cumulative excess returns of machine learning portfolio in the same file\n",
    "\n",
    "file=pd.concat([a_0,cum_sdoc,cum_pca,cum_RF,cum_NN3],axis=1)\n",
    "file.columns=['date','sdoc','pcr','RF','NN3']\n",
    "\n",
    "# Graphical representation of cumulative excess return of machine learning portfolio\n",
    "fig=go.Figure()\n",
    "trace6=go.Scatter(x=file['date'],y=file['sdoc'],name='sDOC')\n",
    "trace7=go.Scatter(x=file['date'],y=file['pcr'],name='PCR')\n",
    "trace8=go.Scatter(x=file['date'],y=file['RF'],name='RF')\n",
    "trace9=go.Scatter(x=file['date'],y=file['NN3'],name='NN3')\n",
    "\n",
    "fig.add_trace(trace6)\n",
    "fig.add_trace(trace7)\n",
    "fig.add_trace(trace8)\n",
    "fig.add_trace(trace9)\n",
    "\n",
    "fig.show()\n",
    "fig.write_image('images/ind13.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa67a5e4",
   "metadata": {},
   "source": [
    "# Graphical representation of DOC-GARCH features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeddfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path\n",
    "\n",
    "path = r'C:/Users/Emile Ndoumbe/Dropbox/garch_month'                     \n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "\n",
    "fig=go.Figure()\n",
    "trace6=go.Scatter(x=file['date'],y=file['X1'],name='garch_doc1')\n",
    "trace7=go.Scatter(x=file['date'],y=file['X2'],name='garch_doc2')\n",
    "trace8=go.Scatter(x=file['date'],y=file['X3'],name='garch_doc3')\n",
    "trace9=go.Scatter(x=file['date'],y=file['X4'],name='garch_doc4')\n",
    "trace10=go.Scatter(x=file['date'],y=file['X5'],name='garch_doc5')\n",
    "fig.add_trace(trace6)\n",
    "fig.add_trace(trace7)\n",
    "fig.add_trace(trace8)\n",
    "fig.add_trace(trace9)\n",
    "fig.add_trace(trace10)\n",
    "fig.show()\n",
    "fig.write_image('images/garchdoc_63_1.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
